{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import spacy\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:  \n",
    "- make the embed non-trainable\n",
    "- try simple cross entropy with start, end\n",
    "- appropriate preprocessing --> lemmatize, stemmatize, (can't remove stop words, or can we?)\n",
    "- try stacked lstms \n",
    "- try bidirectional\n",
    "- use better lr or use lr scheduling\n",
    "- try batch norm\n",
    "- try to visualize the loss (both Lovasz softmax and bin crossentropy) for several examples -- good and bad!\n",
    "- try a combination of both costs, ie, model returns 3 outputs \n",
    "- try to use the padding differently and ignore the pad index\n",
    "- try to make changes to the model --> using conv1d layers\n",
    "- try elmo embeddings --> char lvl, how will that work?\n",
    "- try bert and its modifications --> will need more preprocessing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(row):\n",
    "    z = ['none', 'start']\n",
    "    \n",
    "    temp = row['start_idx']\n",
    "    ans = np.zeros(len(row['text'].split()))\n",
    "    ans[temp] = 1\n",
    "    ans = np.array(ans, dtype=np.int32)\n",
    "    ans = ' '.join([str(z[ans_]) for ans_ in ans])\n",
    "    return ans\n",
    "\n",
    "def foo2(row):\n",
    "    try:\n",
    "        temp = row['start_idx']\n",
    "        temp += len(row['selected_text'].split())\n",
    "        temp -= 1\n",
    "        end_idx = temp\n",
    "        ans = np.zeros(len(row['text'].split()))\n",
    "        ans[temp] = 1\n",
    "    except:\n",
    "        print(row)\n",
    "        import sys; sys.exit(0)\n",
    "    ans = np.array(ans, dtype=np.int32)\n",
    "    z = ['none', 'end']\n",
    "    ans = ' '.join([str(z[ans_]) for ans_ in ans])\n",
    "    return ans\n",
    "\n",
    "def foo3(row):\n",
    "    temp = row['text'].find(row['selected_text'])\n",
    "    if temp>0: \n",
    "        if row['text'][temp-1]!=' ': \n",
    "            temp = row['text'].rfind(' ',0,temp)\n",
    "            if temp==-1: \n",
    "                temp=0\n",
    "    temp = len(row['text'][:temp].split())\n",
    "    return temp\n",
    "\n",
    "def foo4(row):\n",
    "    row_final = []\n",
    "    ele = 'none'\n",
    "#     print(row['start'])\n",
    "    for rows, rowe in zip(row['start'].split(), row['end'].split()):\n",
    "#         print(rows)\n",
    "        if rows == 'start': ele = 'selection'\n",
    "        row_final.append(ele)\n",
    "        if rowe == 'end': ele = 'none'\n",
    "    row_final = ' '.join(row_final)\n",
    "    return row_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rushikej/miniconda3/envs/tf_3/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 27480/27480 [00:01<00:00, 23043.40it/s]\n",
      "100%|██████████| 27480/27480 [00:01<00:00, 26551.43it/s]\n",
      "100%|██████████| 27480/27480 [00:01<00:00, 20131.81it/s]\n",
      "100%|██████████| 27480/27480 [00:00<00:00, 34612.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>selection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>start none none none none none none</td>\n",
       "      <td>none none none none none none end</td>\n",
       "      <td>selection selection selection selection select...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>start none none none none none none none none ...</td>\n",
       "      <td>none end none none none none none none none none</td>\n",
       "      <td>selection selection none none none none none n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>3</td>\n",
       "      <td>none none none start none</td>\n",
       "      <td>none none none none end</td>\n",
       "      <td>none none none selection selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>2</td>\n",
       "      <td>none none start none none</td>\n",
       "      <td>none none none none end</td>\n",
       "      <td>none none selection selection selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>start none none none none none none none none ...</td>\n",
       "      <td>none none end none none none none none none no...</td>\n",
       "      <td>selection selection selection none none none n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  start_idx  \\\n",
       "0  I`d have responded, if I were going   neutral          0   \n",
       "1                             Sooo SAD  negative          0   \n",
       "2                          bullying me  negative          3   \n",
       "3                       leave me alone  negative          2   \n",
       "4                        Sons of ****,  negative          0   \n",
       "\n",
       "                                               start  \\\n",
       "0                start none none none none none none   \n",
       "1  start none none none none none none none none ...   \n",
       "2                          none none none start none   \n",
       "3                          none none start none none   \n",
       "4  start none none none none none none none none ...   \n",
       "\n",
       "                                                 end  \\\n",
       "0                  none none none none none none end   \n",
       "1   none end none none none none none none none none   \n",
       "2                            none none none none end   \n",
       "3                            none none none none end   \n",
       "4  none none end none none none none none none no...   \n",
       "\n",
       "                                           selection  \n",
       "0  selection selection selection selection select...  \n",
       "1  selection selection none none none none none n...  \n",
       "2                 none none none selection selection  \n",
       "3            none none selection selection selection  \n",
       "4  selection selection selection none none none n...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df = pd.read_csv('../tweet-sentiment-extraction/train.csv')\n",
    "\n",
    "if len(df[df['textID']=='fdb77c3752'])!=0:\n",
    "    df = df.drop([314])\n",
    "\n",
    "df['start_idx'] = df.progress_apply(foo3, axis=1)\n",
    "df['start'] = df.progress_apply(foo, axis=1)\n",
    "df['end'] = df.progress_apply(foo2, axis=1)\n",
    "df['selection'] = df.progress_apply(foo4, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1)\n",
    "\n",
    "\n",
    "train_df.to_csv('df_changed_train.csv')\n",
    "val_df.to_csv('df_changed_val.csv')\n",
    "test_df.to_csv('df_changed_test.csv')\n",
    "\n",
    "train_path = 'df_changed_train.csv'\n",
    "test_path = 'df_changed_test.csv'\n",
    "val_path = 'df_changed_val.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = data.Field(sequential=True)\n",
    "# text_id_field = data.Field(sequential=False)\n",
    "\n",
    "labels_field = data.Field(sequential=False)\n",
    "# start_field = data.Field(unk_token=None, pad_token='none')\n",
    "# end_field = data.Field(unk_token=None, pad_token='none')\n",
    "\n",
    "selection_field = data.Field(unk_token=None, pad_token='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields={    # 'textID': ('textID', text_id_field),\n",
    "            'text': ('text', text_field),\n",
    "            \n",
    "            # 'selected_text': ('selected_text', selected_text_field),\n",
    "            'sentiment': ('labels', labels_field), \n",
    "            # 'start': ('start', start_field), \n",
    "            # 'end': ('end', end_field)\n",
    "            \n",
    "            'selection': ('selection', selection_field)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data.TabularDataset.splits(\n",
    "    path='', \n",
    "    train=train_path,\n",
    "    validation=val_path, \n",
    "    test=test_path, \n",
    "    format='csv',\n",
    "    fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 22258\n",
      "Number of validation examples: 2474\n",
      "Number of testing examples: 2748\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(val_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(train_data, \n",
    "                 min_freq = 5,\n",
    "                 vectors = \"glove.6B.100d\",\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "labels_field.build_vocab(train_data)\n",
    "# start_field.build_vocab(train_data)\n",
    "# end_field.build_vocab(train_data)\n",
    "selection_field.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_field.vocab.freqs\n",
    "assert(selection_field.vocab.stoi['none']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: x.text,\n",
    "    sort_within_batch = False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentExtractor(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_dim,\n",
    "                outputs_dim,\n",
    "                embedding_dim,\n",
    "                hidden_dim,\n",
    "                pad_idx,\n",
    "                dropout_rate,\n",
    "                num_layers,\n",
    "                bidirectional\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        if num_layers == 1:\n",
    "            dropout_rate=0\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                            num_layers=num_layers, bidirectional=bidirectional, dropout=dropout_rate)\n",
    "        \n",
    "        # self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim*2, outputs_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, outputs_dim)\n",
    "    \n",
    "    def forward(self, text, sentiment):\n",
    "        # embeddings = self.dropout(self.embedding(text))\n",
    "        embeddings = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embeddings)\n",
    "        \n",
    "        \n",
    "        print(output.shape)\n",
    "        # return self.fc(self.dropout(output))\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(text_field.vocab)\n",
    "outputs_dim = 1\n",
    "hidden_dim = 64\n",
    "num_layers = 1\n",
    "bidirectional = False\n",
    "dropout_rate = 0.25\n",
    "pad_idx = text_field.vocab.stoi[text_field.pad_token]\n",
    "embedding_dim = 100\n",
    "\n",
    "model = SentimentExtractor(input_dim=input_dim,\n",
    "                 outputs_dim=outputs_dim,\n",
    "                 hidden_dim=hidden_dim,\n",
    "                 num_layers=num_layers,\n",
    "                 bidirectional=bidirectional,\n",
    "                 dropout_rate=dropout_rate,\n",
    "                 embedding_dim=embedding_dim,\n",
    "                 pad_idx=pad_idx)\n",
    "\n",
    "# tag_pad_idx = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=tag_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "model.apply(init_weights);\n",
    "\n",
    "# print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "pretrained_embeddings = text_field.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings);\n",
    "\n",
    "model.embedding.weight.data[1] = torch.zeros(embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 32])\n",
      "tensor([  85,   70,  972,   44,    0,  556,    6,    3,  389, 1823,    3, 1764,\n",
      "         209, 1963,    8,    0,  144,  241,   26,    0,   10,  152,    8,    0,\n",
      "          71, 3063, 4047,    0,    0,   93, 1204,  915], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in train_iterator:\n",
    "        text = batch.text\n",
    "        # preds = model(text)\n",
    "        print(text.shape)\n",
    "        print(text[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_categorical_accuracy(y_pred, y_true, tag_pad_idx):\n",
    "#     y_pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "#     non_pad_element_idxs = (y_true!=tag_pad_idx).nonzero()\n",
    "#     correct = y_pred[non_pad_element_idxs].squeeze(1).eq(y_true[non_pad_element_idxs])\n",
    "#     return correct.sum() / torch.FloatTensor([y_true[non_pad_element_idxs].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1: # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * Variable(signs))\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, loss_func, optimizer):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    model.to(device)\n",
    "    with torch.enable_grad():\n",
    "#     with torch.no_grad():\n",
    "        model.train()\n",
    "#         model.eval()\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "#             print('Text shape: ', text.shape)\n",
    "\n",
    "            preds = model(batch.text)\n",
    "#             print(preds.shape)\n",
    "#             print(preds)\n",
    "#             print('\\n\\n\\n\\n')\n",
    "            preds = preds.view(-1)\n",
    "            true = batch.selection\n",
    "#             print(true.shape)\n",
    "#             print(true)\n",
    "            true = true.view(-1)\n",
    "            \n",
    "            j_loss = loss_func(preds, true)\n",
    "            j_loss.backward()\n",
    "#             print(j_loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += j_loss.item()\n",
    "#             break\n",
    "        \n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_one_epoch(model, iterator, loss_func):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "#             print('Text shape: ', text.shape)\n",
    "\n",
    "            preds = model(batch.text)\n",
    "#             print(preds.shape)\n",
    "            preds = preds.view(-1)\n",
    "            true = batch.selection\n",
    "            true = true.view(-1)\n",
    "\n",
    "            j_loss = loss_func(preds, true)\n",
    "            \n",
    "            epoch_loss += j_loss.item()\n",
    "\n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.055\n",
      "Val Loss: 1.051\n",
      "Epoch: 02 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.037\n",
      "Val Loss: 1.036\n",
      "Epoch: 03 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.028\n",
      "Val Loss: 1.031\n",
      "Epoch: 04 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.023\n",
      "Val Loss: 1.027\n",
      "Epoch: 05 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.019\n",
      "Val Loss: 1.024\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.016\n",
      "Val Loss: 1.020\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.012\n",
      "Val Loss: 1.018\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.010\n",
      "Val Loss: 1.015\n",
      "Epoch: 09 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.008\n",
      "Val Loss: 1.013\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.006\n",
      "Val Loss: 1.011\n",
      "Epoch: 11 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.004\n",
      "Val Loss: 1.008\n",
      "Epoch: 12 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.002\n",
      "Val Loss: 1.006\n",
      "Epoch: 13 | Epoch Time: 0m 3s\n",
      "Train Loss: 1.000\n",
      "Val Loss: 1.004\n",
      "Epoch: 14 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.998\n",
      "Val Loss: 1.003\n",
      "Epoch: 15 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.997\n",
      "Val Loss: 1.001\n",
      "Epoch: 16 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.995\n",
      "Val Loss: 0.999\n",
      "Epoch: 17 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.993\n",
      "Val Loss: 0.998\n",
      "Epoch: 18 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.992\n",
      "Val Loss: 0.996\n",
      "Epoch: 19 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.989\n",
      "Val Loss: 0.994\n",
      "Epoch: 20 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.988\n",
      "Val Loss: 0.992\n",
      "Epoch: 21 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.986\n",
      "Val Loss: 0.992\n",
      "Epoch: 22 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.983\n",
      "Val Loss: 0.988\n",
      "Epoch: 23 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.981\n",
      "Val Loss: 0.987\n",
      "Epoch: 24 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.978\n",
      "Val Loss: 0.984\n",
      "Epoch: 25 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.975\n",
      "Val Loss: 0.983\n",
      "Epoch: 26 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.971\n",
      "Val Loss: 0.979\n",
      "Epoch: 27 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.968\n",
      "Val Loss: 0.978\n",
      "Epoch: 28 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.965\n",
      "Val Loss: 0.974\n",
      "Epoch: 29 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.961\n",
      "Val Loss: 0.973\n",
      "Epoch: 30 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.958\n",
      "Val Loss: 0.969\n",
      "Epoch: 31 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.953\n",
      "Val Loss: 0.967\n",
      "Epoch: 32 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.952\n",
      "Val Loss: 0.964\n",
      "Epoch: 33 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.947\n",
      "Val Loss: 0.961\n",
      "Epoch: 34 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.943\n",
      "Val Loss: 0.959\n",
      "Epoch: 35 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.940\n",
      "Val Loss: 0.957\n",
      "Epoch: 36 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.939\n",
      "Val Loss: 0.955\n",
      "Epoch: 37 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.935\n",
      "Val Loss: 0.954\n",
      "Epoch: 38 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.933\n",
      "Val Loss: 0.953\n",
      "Epoch: 39 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.929\n",
      "Val Loss: 0.952\n",
      "Epoch: 40 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.926\n",
      "Val Loss: 0.951\n",
      "Epoch: 41 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.925\n",
      "Val Loss: 0.952\n",
      "Epoch: 42 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.922\n",
      "Val Loss: 0.951\n",
      "Epoch: 43 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.919\n",
      "Val Loss: 0.953\n",
      "Epoch: 44 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.917\n",
      "Val Loss: 0.955\n",
      "Epoch: 45 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.914\n",
      "Val Loss: 0.951\n",
      "Epoch: 46 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.912\n",
      "Val Loss: 0.961\n",
      "Epoch: 47 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.913\n",
      "Val Loss: 0.948\n",
      "Epoch: 48 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.908\n",
      "Val Loss: 0.956\n",
      "Epoch: 49 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.903\n",
      "Val Loss: 0.949\n",
      "Epoch: 50 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.904\n",
      "Val Loss: 0.971\n",
      "Epoch: 51 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.906\n",
      "Val Loss: 0.947\n",
      "Epoch: 52 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.898\n",
      "Val Loss: 0.962\n",
      "Epoch: 53 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.894\n",
      "Val Loss: 0.948\n",
      "Epoch: 54 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.894\n",
      "Val Loss: 0.974\n",
      "Epoch: 55 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.892\n",
      "Val Loss: 0.947\n",
      "Epoch: 56 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.892\n",
      "Val Loss: 0.958\n",
      "Epoch: 57 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.882\n",
      "Val Loss: 0.955\n",
      "Epoch: 58 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.882\n",
      "Val Loss: 0.960\n",
      "Epoch: 59 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.879\n",
      "Val Loss: 0.959\n",
      "Epoch: 60 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.877\n",
      "Val Loss: 0.959\n",
      "Epoch: 61 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.874\n",
      "Val Loss: 0.954\n",
      "Epoch: 62 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.877\n",
      "Val Loss: 0.959\n",
      "Epoch: 63 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.890\n",
      "Val Loss: 0.945\n",
      "Epoch: 64 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.883\n",
      "Val Loss: 0.963\n",
      "Epoch: 65 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.885\n",
      "Val Loss: 0.949\n",
      "Epoch: 66 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.885\n",
      "Val Loss: 0.958\n",
      "Epoch: 67 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.864\n",
      "Val Loss: 0.971\n",
      "Epoch: 68 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.866\n",
      "Val Loss: 0.963\n",
      "Epoch: 69 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.864\n",
      "Val Loss: 0.961\n",
      "Epoch: 70 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.867\n",
      "Val Loss: 0.945\n",
      "Epoch: 71 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.860\n",
      "Val Loss: 0.974\n",
      "Epoch: 72 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.856\n",
      "Val Loss: 0.950\n",
      "Epoch: 73 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.851\n",
      "Val Loss: 0.976\n",
      "Epoch: 74 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.850\n",
      "Val Loss: 0.969\n",
      "Epoch: 75 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.851\n",
      "Val Loss: 0.982\n",
      "Epoch: 76 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.849\n",
      "Val Loss: 0.964\n",
      "Epoch: 77 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.845\n",
      "Val Loss: 0.979\n",
      "Epoch: 78 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.842\n",
      "Val Loss: 0.967\n",
      "Epoch: 79 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.842\n",
      "Val Loss: 0.981\n",
      "Epoch: 80 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.840\n",
      "Val Loss: 0.955\n",
      "Epoch: 81 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.840\n",
      "Val Loss: 0.976\n",
      "Epoch: 82 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.839\n",
      "Val Loss: 0.982\n",
      "Epoch: 83 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.833\n",
      "Val Loss: 0.979\n",
      "Epoch: 84 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.837\n",
      "Val Loss: 0.975\n",
      "Epoch: 85 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.829\n",
      "Val Loss: 0.983\n",
      "Epoch: 86 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.828\n",
      "Val Loss: 0.962\n",
      "Epoch: 87 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.826\n",
      "Val Loss: 0.995\n",
      "Epoch: 88 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.824\n",
      "Val Loss: 0.998\n",
      "Epoch: 89 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.827\n",
      "Val Loss: 0.991\n",
      "Epoch: 90 | Epoch Time: 0m 2s\n",
      "Train Loss: 0.823\n",
      "Val Loss: 0.960\n",
      "Epoch: 91 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.822\n",
      "Val Loss: 0.996\n",
      "Epoch: 92 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.817\n",
      "Val Loss: 0.961\n",
      "Epoch: 93 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.816\n",
      "Val Loss: 0.998\n",
      "Epoch: 94 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.811\n",
      "Val Loss: 1.000\n",
      "Epoch: 95 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.820\n",
      "Val Loss: 0.978\n",
      "Epoch: 96 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.820\n",
      "Val Loss: 1.027\n",
      "Epoch: 97 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.836\n",
      "Val Loss: 1.004\n",
      "Epoch: 98 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.819\n",
      "Val Loss: 1.004\n",
      "Epoch: 99 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.808\n",
      "Val Loss: 0.979\n",
      "Epoch: 100 | Epoch Time: 0m 3s\n",
      "Train Loss: 0.800\n",
      "Val Loss: 0.994\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_iterator, lovasz_hinge_flat, optimizer)\n",
    "    val_loss = val_one_epoch(model, valid_iterator, lovasz_hinge_flat)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_time_min, epoch_time_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_time_min}m {epoch_time_secs}s')\n",
    "    print('Train Loss: {:.3f}'.format(train_loss))\n",
    "    print('Val Loss: {:.3f}'.format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(100,1000)\n",
    "# y = torch.randn(1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.to(device); y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = torch.matmul(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn((32,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.nonzero().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input_ = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# print(input_); print(target)\n",
    "# output = loss(input_, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_loss(true, pred):\n",
    "#     # this is the new criterion\n",
    "    \n",
    "#     # expected true to be of shape (batch_size, None) -- but only 0, 1\n",
    "#     # expecting pred to be of shape (batch_size, None) -- but float values\n",
    "    \n",
    "#     true_start, true_end = true\n",
    "#     pred_start, pred_end = pred\n",
    "    \n",
    "#     pred_start_idx = (pred_start==1).nonzero()\n",
    "#     pred_end_idx = (pred_end==1).nonzero()\n",
    "    \n",
    "#     if pred_start_idx > pred_end_idx: return 100\n",
    "    \n",
    "#     true_start_idx = (true_start==1).nonzero()\n",
    "#     true_end_idx = (true_end==1).nonzero()\n",
    "    \n",
    "#     I = torch.min(true_end_idx, pred_end_idx) - torch.max(true_start_idx, pred_start_idx)\n",
    "#     if I < 0: return 1\n",
    "#     P = pred_end_idx - pred_start_idx\n",
    "#     T = true_end_idx - true_start_idx\n",
    "    \n",
    "#     smooth = 1e-6\n",
    "    \n",
    "#     loss = 1 - ((I + smooth) / (P + T - I + smooth))\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in valid_iterator:\n",
    "#     text = batch.text\n",
    "#     print(text)\n",
    "#     pred = model(text)\n",
    "#     pred = torch.transpose\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import Variable\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def lovasz_grad(gt_sorted):\n",
    "#     \"\"\"\n",
    "#     Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "#     See Alg. 1 in paper\n",
    "#     \"\"\"\n",
    "#     p = len(gt_sorted)\n",
    "#     gts = gt_sorted.sum()\n",
    "#     intersection = gts - gt_sorted.float().cumsum(0)\n",
    "#     union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "#     jaccard = 1. - intersection / union\n",
    "#     if p > 1: # cover 1-pixel case\n",
    "#         jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "#     return jaccard\n",
    "\n",
    "# def lovasz_hinge_flat(logits, labels):\n",
    "#     \"\"\"\n",
    "#     Binary Lovasz hinge loss\n",
    "#       logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "#       labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "#       ignore: label to ignore\n",
    "#     \"\"\"\n",
    "#     if len(labels) == 0:\n",
    "#         # only void pixels, the gradients should be 0\n",
    "#         return logits.sum() * 0.\n",
    "#     signs = 2. * labels.float() - 1.\n",
    "#     errors = (1. - logits * Variable(signs))\n",
    "#     errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "#     perm = perm.data\n",
    "#     gt_sorted = labels[perm]\n",
    "#     grad = lovasz_grad(gt_sorted)\n",
    "#     loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = torch.FloatTensor(2,3)\n",
    "# a = [[1,0,1],[0,1,1]]\n",
    "# a = np.array([np.array(aa,dtype=np.float32) for aa in a])\n",
    "# x2 = torch.tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = x1.view(-1)\n",
    "# x2 = x2.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lovasz_hinge_flat(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_3]",
   "language": "python",
   "name": "conda-env-tf_3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
