{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "folder = '../outputs/exp2/finetune_roberta_lovasz/'\n",
    "device = torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tokenizers\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pkl\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, max_len=96):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = 'selected_text' in df\n",
    "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "            vocab_file='../input/roberta-base/vocab.json', \n",
    "            merges_file='../input/roberta-base/merges.txt', \n",
    "            lowercase=True,\n",
    "            add_prefix_space=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {}\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        ids, masks, tweet, offsets, selected_text = self.get_input_data(row)\n",
    "        data['ids'] = ids\n",
    "        data['masks'] = masks\n",
    "        data['tweet'] = tweet\n",
    "        data['offsets'] = offsets\n",
    "        data['selected_text'] = selected_text\n",
    "        \n",
    "        if self.labeled:\n",
    "            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
    "            data['target_mask'] = self.get_target_mask(row, tweet, offsets)\n",
    "            data['start_idx'] = start_idx\n",
    "            data['end_idx'] = end_idx\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get_input_data(self, row):\n",
    "        tweet = \" \" + \" \".join(row.text.lower().split())\n",
    "        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n",
    "        encoding = self.tokenizer.encode(tweet)\n",
    "        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n",
    "        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n",
    "        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
    "                \n",
    "        pad_len = self.max_len - len(ids)\n",
    "        if pad_len > 0:\n",
    "            ids += [1] * pad_len\n",
    "            offsets += [(0, 0)] * pad_len\n",
    "        \n",
    "        ids = torch.tensor(ids)\n",
    "        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
    "        offsets = torch.tensor(offsets)\n",
    "        \n",
    "        return ids, masks, tweet, offsets, selected_text\n",
    "        \n",
    "    def get_target_idx(self, row, tweet, offsets):\n",
    "        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "        len_st = len(selected_text) - 1\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "\n",
    "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "            if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st - 1\n",
    "                break\n",
    "\n",
    "        char_targets = [0] * len(tweet)\n",
    "        if idx0 != None and idx1 != None:\n",
    "            for ct in range(idx0, idx1 + 1):\n",
    "                char_targets[ct] = 1\n",
    "\n",
    "        target_idx = []\n",
    "        for j, (offset1, offset2) in enumerate(offsets):\n",
    "            if sum(char_targets[offset1: offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "\n",
    "        start_idx = target_idx[0]\n",
    "        end_idx = target_idx[-1]\n",
    "        \n",
    "        return start_idx, end_idx\n",
    "    \n",
    "    def get_target_mask(self, row, tweet, offsets):\n",
    "        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "        len_st = len(selected_text) - 1\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "\n",
    "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "            if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st - 1\n",
    "                break\n",
    "\n",
    "        char_targets = [0] * len(tweet)\n",
    "        if idx0 != None and idx1 != None:\n",
    "            for ct in range(idx0, idx1 + 1):\n",
    "                char_targets[ct] = 1\n",
    "\n",
    "        target_idx = []\n",
    "        for j, (offset1, offset2) in enumerate(offsets):\n",
    "            if sum(char_targets[offset1: offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "\n",
    "        start_idx = target_idx[0]\n",
    "        end_idx = target_idx[-1]\n",
    "        \n",
    "        target_mask = np.array([0.]*self.max_len)\n",
    "        target_mask[start_idx:end_idx+1] = 1.\n",
    "        \n",
    "        return target_mask\n",
    "        \n",
    "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "def get_test_loader(df, batch_size=32):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TweetModel, self).__init__()\n",
    "\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            '../input/roberta-base/config.json', output_hidden_states=True)    \n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            '../input/roberta-base/pytorch_model.bin', config=config)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(config.hidden_size, 2)\n",
    "        self.fc2 = nn.Linear(config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "        \n",
    "        nn.init.normal_(self.fc2.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc2.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        lhs, _, hs = self.roberta(input_ids, attention_mask)\n",
    "         \n",
    "#         x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
    "#         x = torch.mean(x, 0)\n",
    "        x = lhs\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        pred_masks = self.sigmoid(x)\n",
    "#         start_logits, end_logits = x.split(1, dim=-1)\n",
    "#         start_logits = start_logits.squeeze(-1)\n",
    "#         end_logits = end_logits.squeeze(-1)\n",
    "                \n",
    "        return pred_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "#     ce_loss = nn.CrossEntropyLoss()\n",
    "#     start_loss = ce_loss(start_logits, start_positions)\n",
    "#     end_loss = ce_loss(end_logits, end_positions)    \n",
    "#     total_loss = start_loss + end_loss\n",
    "#     return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn2(pred_masks, target_masks):\n",
    "#     loss = nn.BCELoss()(pred_masks, target_masks)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jaccard_loss(pred, target, smooth=1e-10):\n",
    "#     if len(pred.shape)>=3:\n",
    "#         pred = pred.view(pred.shape[0], pred.shape[1])\n",
    "#     I = (pred * target).sum()\n",
    "#     P = pred.sum()\n",
    "#     T = target.sum()\n",
    "#     loss = 1 - ((I + smooth) / (P + T - I + smooth))\n",
    "#     return loss\n",
    "\n",
    "# def dice_loss(pred, target, smooth=1e-10):\n",
    "#     if len(pred.shape)>=3:\n",
    "#         pred = pred.view(pred.shape[0], pred.shape[1])\n",
    "#     I = (pred * target).sum()\n",
    "#     P = pred.sum()\n",
    "#     T = target.sum()\n",
    "#     loss = 1 - (2 * (I + smooth) / (P + T + smooth))\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1: # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "def lovasz_softmax_flat(probas, labels, classes='present'):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "    \"\"\"\n",
    "    if probas.numel() == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return probas * 0.\n",
    "    C = probas.size(1)\n",
    "    losses = []\n",
    "    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n",
    "    for c in class_to_sum:\n",
    "        fg = (labels == c).float() # foreground for class c\n",
    "        if (classes is 'present' and fg.sum() == 0):\n",
    "            continue\n",
    "        if C == 1:\n",
    "            if len(classes) > 1:\n",
    "                raise ValueError('Sigmoid output possible only with 1 class')\n",
    "            class_pred = probas[:, 0]\n",
    "        else:\n",
    "            class_pred = probas[:, c]\n",
    "        errors = (Variable(fg) - class_pred).abs()\n",
    "        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "        perm = perm.data\n",
    "        fg_sorted = fg[perm]\n",
    "        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n",
    "    return mean(losses)\n",
    "\n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = torch.randn(32,96,1).view(-1, 1)\n",
    "# t = torch.zeros(32, 96).view(-1)\n",
    "# lovasz_softmax_flat(p, t, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn3(pred_masks, target_masks):\n",
    "    loss = lovasz_softmax_flat(pred_masks.view(-1,1), target_masks.view(-1), [1])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_text(text, start_idx, end_idx, offsets):\n",
    "    selected_text = \"\"\n",
    "    for ix in range(start_idx, end_idx + 1):\n",
    "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
    "            selected_text += \" \"\n",
    "    return selected_text\n",
    "\n",
    "def get_selected_text2(text, idxs, offsets):\n",
    "    selected_text = \"\"\n",
    "    for ix in idxs:\n",
    "#         print('ix:', ix)\n",
    "#         print('offsets shape:', offsets.shape)\n",
    "#         print('offsets[ix]:', offsets[ix])\n",
    "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
    "            selected_text += \" \"\n",
    "    return selected_text\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
    "    start_pred = np.argmax(start_logits)\n",
    "    end_pred = np.argmax(end_logits)\n",
    "    if start_pred > end_pred:\n",
    "        pred = text\n",
    "    else:\n",
    "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
    "        \n",
    "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
    "    return jaccard(true, pred)\n",
    "\n",
    "def compute_jaccard_score2(text, target_masks, pred_masks, offsets):\n",
    "    idxs = np.array(np.where(pred_masks>0.5))[0].T\n",
    "    if len(idxs)==0: \n",
    "        js = 0\n",
    "    else:\n",
    "        pred = get_selected_text2(text, idxs, offsets)\n",
    "        idxs_true = np.array(np.where(target_masks==1))[0].T\n",
    "        true = get_selected_text2(text, idxs_true, offsets)\n",
    "        js = jaccard(true,pred)\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = torch.randn(3,9,1)\n",
    "# p = nn.Sigmoid()(p)\n",
    "# i = (p>0.5).nonzero()\n",
    "# i = torch.where(p>1)\n",
    "# p[i[0]].shape\n",
    "# p[i]\n",
    "# i = torch.stack(i).T\n",
    "# len(i)\n",
    "# p[i[0]][i[1]], i, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# math.isfinite([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "# a.type(torch.float32)\n",
    "# # a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename, device):\n",
    "    model.cuda(device)\n",
    "    metrics = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch: {epoch}')\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_jaccard = 0.0\n",
    "            total = (len(dataloaders_dict[phase]))\n",
    "            t = tqdm(total=total)\n",
    "            print_every=10\n",
    "                        \n",
    "            for j, data in enumerate(dataloaders_dict[phase]):\n",
    "                ids = data['ids'].cuda(device)\n",
    "                masks = data['masks'].cuda(device)\n",
    "                tweet = data['tweet']\n",
    "                offsets = data['offsets'].numpy()\n",
    "                target_masks = (  data['target_mask'].type(torch.float32)  ).cuda(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    pred_masks = model(ids, masks)\n",
    "#                     pred_masks = pred_masks.reshape(pred_masks.shape[0], pre)\n",
    "#                     global pred_masks_, target_masks_\n",
    "#                     pred_masks_ = pred_masks; target_masks_ = target_masks\n",
    "#                     return 0\n",
    "                    loss = criterion(pred_masks, target_masks)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()             \n",
    "            \n",
    "                    epoch_loss += loss.item() * len(ids)        \n",
    "                    target_masks = target_masks.cpu().detach().numpy()\n",
    "                    pred_masks = pred_masks.cpu().detach().numpy()\n",
    "    \n",
    "                    for i in range(len(ids)):                        \n",
    "                        jaccard_score_ = compute_jaccard_score2(\n",
    "                                tweet[i],\n",
    "                                target_masks[i],\n",
    "                                pred_masks[i],\n",
    "    #                             start_idx[i],\n",
    "    #                             end_idx[i],\n",
    "    #                             start_logits[i], \n",
    "    #                             end_logits[i], \n",
    "                                offsets[i])\n",
    "                        epoch_jaccard += jaccard_score_\n",
    "#                 print(i%print_every==0)\n",
    "                if j%print_every==0:\n",
    "#                     print('!')\n",
    "                    t.update(print_every if j+print_every<total else total-j)\n",
    "            t.close(); \n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n",
    "                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n",
    "            \n",
    "            metrics.append([phase, epoch_loss, epoch_jaccard])\n",
    "            with open(folder+'metrics.pkl', 'wb') as f:\n",
    "                pkl.dump(metrics, f)\n",
    "            \n",
    "            \n",
    "        torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 48\n",
    "\n",
    "# %%time\n",
    "# train_file = '../input/tweet-sentiment-extraction/train.csv'\n",
    "train_file = '../tweet-sentiment-extraction/train.csv'\n",
    "train_df = pd.read_csv(train_file)\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "train_df['selected_text'] = train_df['selected_text'].astype(str)\n",
    "# train_df = train_df[train_df['sentiment']!='neutral']\n",
    "\n",
    "model = TweetModel()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
    "#     print(f'Fold: {fold}')\n",
    "\n",
    "#     model = TweetModel()\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "#     criterion = loss_fn2    \n",
    "#     dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "#     device = torch.device('cuda:2')\n",
    "# #     train_model(\n",
    "# #         model, \n",
    "# #         dataloaders_dict,\n",
    "# #         criterion, \n",
    "# #         optimizer, \n",
    "# #         num_epochs,\n",
    "# #         folder+f'roberta_fold{fold}.pth',\n",
    "# #         device)\n",
    "    \n",
    "#     model.cuda(device)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f'Epoch: {epoch}')\n",
    "#         for phase in ['train', 'val']:\n",
    "#             if phase == 'train':\n",
    "#                 model.train()\n",
    "#             else:\n",
    "#                 model.eval()\n",
    "\n",
    "#             epoch_loss = 0.0\n",
    "#             epoch_jaccard = 0.0\n",
    "#             total = (len(dataloaders_dict[phase]))\n",
    "#             t = tqdm(total=total)\n",
    "#             print_every=10\n",
    "                        \n",
    "#             for j, data in enumerate(dataloaders_dict[phase]):\n",
    "#                 ids = data['ids'].cuda(device)\n",
    "#                 masks = data['masks'].cuda(device)\n",
    "#                 tweet = data['tweet']\n",
    "#                 offsets = data['offsets'].numpy()\n",
    "# #                 start_idx = torch.tensor( data['start_idx'] ).cuda(device)\n",
    "# #                 end_idx = data['end_idx'].cuda(device)\n",
    "# #                 selected_text = data['selected_text']\n",
    "#                 target_masks = (  data['target_mask'].type(torch.float32)  ).cuda(device)\n",
    "#     #             start_logits, end_logits = model(ids, masks)\n",
    "\n",
    "#     #             loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
    "#                 optimizer.zero_grad()\n",
    "                \n",
    "#                 with torch.set_grad_enabled(phase == 'train'):\n",
    "#                     pred_masks = model(ids, masks)\n",
    "\n",
    "#                     loss = criterion(pred_masks, target_masks)\n",
    "#                     if phase == 'train':\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()             \n",
    "            \n",
    "#                     epoch_loss += loss.item() * len(ids)\n",
    "                    \n",
    "#                     target_masks = target_masks.cpu().detach().numpy()\n",
    "#                     pred_masks = pred_masks.cpu().detach().numpy()\n",
    "    \n",
    "#                     for i in range(len(ids)):\n",
    "#                         text = tweet[i]\n",
    "#                         target_masks_ = target_masks[i]\n",
    "#                         pred_masks_ = pred_masks[i].reshape(pred_masks[i].shape[0])\n",
    "#                         offsets_ = offsets[i]\n",
    "                            \n",
    "#                         idxs = np.array(np.where(pred_masks_>0.5))[0].T\n",
    "#                         if len(idxs)==0: \n",
    "#                             js = 0\n",
    "#                         else:\n",
    "#                             pred = get_selected_text2(text, idxs, offsets_)\n",
    "# #                             print(idxs)\n",
    "#                             idxs_true = np.array(np.where(target_masks_==1))[0].T\n",
    "#                             true = get_selected_text2(text, idxs_true, offsets_)\n",
    "#                             js = jaccard(true,pred)\n",
    "#                         jaccard_score = js\n",
    "#                         epoch_jaccard += jaccard_score\n",
    "# #                 print(i%print_every==0)\n",
    "#                 break\n",
    "#                 if j%print_every==0:\n",
    "# #                     print('!')\n",
    "#                     t.update(print_every if j+print_every<total else total-j)\n",
    "                \n",
    "#             t.close(); \n",
    "#             epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "#             epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "#             print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n",
    "#                 epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n",
    "            \n",
    "# #         torch.save(model.state_dict(), filename)\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "# skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
    "#     break\n",
    "# for fold, (train_idx, val_idx) in enumerate((train_df, train_df.sentiment), start=1): \n",
    "#     print(f'Fold: {fold}')\n",
    "\n",
    "model = TweetModel()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "criterion = loss_fn3    \n",
    "dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "\n",
    "train_model(\n",
    "    model, \n",
    "    dataloaders_dict,\n",
    "    criterion, \n",
    "    optimizer, \n",
    "    num_epochs,\n",
    "    folder+f'roberta_fold{fold}.pth',\n",
    "    device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min( (optimizer.param_groups[0])['params'][2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(203):\n",
    "# #     print(torch.min( (optimizer.param_groups[0])['params'][i] ))\n",
    "# #     print(torch.max( (optimizer.param_groups[0])['params'][i] ))\n",
    "# #     print((optimizer.param_groups[0])['params'][i].shape )\n",
    "#     print(torch.sum(torch.isnan(optimizer.param_groups[0]['params'][i])).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((optimizer.param_groups[0])['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (pred_masks_.view(pred_masks_.shape[0], pred_masks_.shape[1]) * target_masks_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.FloatTensor(3,5)\n",
    "q = torch.FloatTensor(3,5)\n",
    "r = p*q\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
    "#     print(train_idx, val_idx)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred_masks.cpu().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(target_masks.cpu().shape)\n",
    "# torch.unique(target_masks.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.tensor(torch.FloatTensor(3,9))\n",
    "# t = nn.Sigmoid()(t)\n",
    "# t = t>0.5\n",
    "# t = t.type(torch.float32)\n",
    "# p = torch.tensor(torch.FloatTensor(3,9))\n",
    "# p = nn.Sigmoid()(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.BCELoss()(p, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(pred_masks)\n",
    "# torch.min(pred_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = RobertaConfig.from_pretrained(\n",
    "#             '../input/roberta-base/config.json', output_hidden_states=True)    \n",
    "# config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhs[0][:10][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.mean(x, 0)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sigmoid()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "# test_df['text'] = test_df['text'].astype(str)\n",
    "# test_loader = get_test_loader(test_df)\n",
    "# predictions = []\n",
    "# models = []\n",
    "# for fold in range(skf.n_splits):\n",
    "#     model = TweetModel()\n",
    "#     model.cuda()\n",
    "#     model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n",
    "#     model.eval()\n",
    "#     models.append(model)\n",
    "\n",
    "# for data in test_loader:\n",
    "#     ids = data['ids'].cuda()\n",
    "#     masks = data['masks'].cuda()\n",
    "#     tweet = data['tweet']\n",
    "#     offsets = data['offsets'].numpy()\n",
    "\n",
    "#     start_logits = []\n",
    "#     end_logits = []\n",
    "#     for model in models:\n",
    "#         with torch.no_grad():\n",
    "#             output = model(ids, masks)\n",
    "#             start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "#             end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "\n",
    "#     start_logits = np.mean(start_logits, axis=0)\n",
    "#     end_logits = np.mean(end_logits, axis=0)\n",
    "#     for i in range(len(ids)):    \n",
    "#         start_pred = np.argmax(start_logits[i])\n",
    "#         end_pred = np.argmax(end_logits[i])\n",
    "#         if start_pred > end_pred:\n",
    "#             pred = tweet[i]\n",
    "#         else:\n",
    "#             pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
    "#         predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "# sub_df['selected_text'] = predictions\n",
    "# sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
    "# sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
    "# sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
    "# sub_df.to_csv('submission.csv', index=False)\n",
    "# sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm_notebook\n",
    "# import time\n",
    "# t2 = tqdm_notebook(total=10)\n",
    "# for j in range(10):\n",
    "#     t = tqdm_notebook(total=100)\n",
    "#     for i in range(100):\n",
    "#         time.sleep(0.01)\n",
    "#         if i%12==0: \n",
    "#             if 100-i>12: t.update(12)\n",
    "#             else: t.update(100-i)\n",
    "#     t.close(); del t\n",
    "#     t2.update(1)\n",
    "# t2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# import time\n",
    "# import sys\n",
    "\n",
    "# t2 = tqdm(total=10, file=sys.stdout)\n",
    "# for j in range(10):\n",
    "#     t3 = tqdm(total=100, file=sys.stdout, disable=True)\n",
    "#     for i in range(100):\n",
    "#         time.sleep(0.01)\n",
    "#         if i%12==0: \n",
    "#             if 100-i>12: t3.update(12)\n",
    "#             else: t3.update(100-i)\n",
    "#             t3.refresh()\n",
    "#     t3.close(); del t3\n",
    "#     t2.update(1)\n",
    "# t2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=48\n",
    "# dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "# tl, vl = dataloaders_dict.values()\n",
    "# len(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# a = torch.FloatTensor(3,5)\n",
    "# a.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}